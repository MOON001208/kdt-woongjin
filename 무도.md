당신은 데이터 엔지니어 + 데이터 분석가 + Airflow 아키텍트 역할을 동시에 수행하는 AI입니다.
본 작업은 “Sequential Thinking MCP” 방식으로 진행되며, 반드시 아래 순서를 엄격히 지켜 사고하고 산출해야 합니다.
중간 단계를 생략하거나 순서를 바꾸지 마세요.

==============================
[프로젝트 컨텍스트]
==============================
- 프로젝트명: 무한도전 레전드 아카이브
- 목적: YouTube Data API v3를 활용해 무한도전 관련 영상 데이터를 30분 주기로 수집하고,
         Supabase에 중복 없이 적재하며, 향후 분석/시각화를 위한 데이터 파이프라인 구축
- 사용 기술: YouTube Data API v3, Python, Airflow, Supabase(PostgreSQL)

==============================
[절대 변경 불가 조건]
==============================
1. YouTube API로 수집해야 할 데이터 컬럼은 반드시 아래와 같아야 한다.

{
    'channel_id': item['snippet']['channelId'],
    'video_id': item['id'],
    'title': item['snippet']['title'],
    'description': item['snippet']['description'],
    'thumbnail_url': item['snippet']['thumbnails']['high']['url'],
    'view_count': item['statistics'].get('viewCount', '0'),
    'like_count': item['statistics'].get('likeCount', '0'),
    'comment_count': item['statistics'].get('commentCount', '0'),
    'published_at': item['snippet']['publishedAt'],
    'collected_at': 현재 시각 (YYYY-MM-DD HH:MM:SS)
}

2. 데이터 수집 주기는 반드시 **30분마다 실행**되어야 한다.
3. 이전에 수집한 video_id는 **절대 중복 적재되지 않아야 한다.**
4. Supabase 스키마 및 테이블 생성 SQL은 아래 정의를 그대로 사용해야 한다.

CREATE SCHEMA IF NOT EXISTS tlswlgo3;

CREATE TABLE IF NOT EXISTS tlswlgo3.youtube_videos (
    video_id TEXT PRIMARY KEY,
    channel_id TEXT,
    channel_title TEXT,
    description TEXT,
    thumbnail_url TEXT,
    view_count BIGINT,
    like_count BIGINT,
    comment_count BIGINT,
    published_at TIMESTAMP,
    collected_at TIMESTAMP
);

5. 전체 코드는 **Airflow DAG 형식**으로 작성되어야 한다.

==============================
[Sequential Thinking 단계]
==============================

### STEP 1. 데이터 수집 전략 설계
- YouTube Data API v3에서 Search API + Videos API를 어떻게 조합할지 설계
- 어떤 키워드 전략으로 무한도전 영상을 수집할지 명확히 정의
- API 호출 제한을 고려한 페이지네이션 전략 수립

[산출물]
- 데이터 수집 흐름 다이어그램(텍스트)
- API 호출 순서 설명

---

### STEP 2. 중복 방지 로직 설계
- Supabase에 이미 존재하는 video_id를 기준으로 중복 수집을 방지하는 로직 설계
- DB 조회 기반 중복 체크 방식 명시

[산출물]
- 중복 방지 알고리즘 설명
- SQL 또는 Python 레벨에서의 중복 처리 전략

---

### STEP 3. Supabase 연결 및 테이블 생성 코드 작성
- psycopg2 또는 SQLAlchemy 기반 Supabase 연결
- 제공된 스키마/테이블 생성 SQL을 실행하는 Python 코드 작성

[산출물]
- Supabase 연결 코드
- schema + table 생성 코드

---

### STEP 4. YouTube 데이터 수집 Python 함수 구현
- YouTube API 인증
- Search 결과 수집
- Videos API를 통한 statistics 병합
- 최종 데이터 구조를 조건에 맞게 정제

[산출물]
- 단일 실행 가능한 Python 수집 함수

---

### STEP 5. Supabase 적재(Load) 로직 구현
- INSERT 시 PRIMARY KEY(video_id) 기준 중복 발생 시 무시
- 트랜잭션 처리 포함

[산출물]
- Supabase 적재 함수
- 예외 처리 로직

---

### STEP 6. Airflow DAG 구성
- DAG 이름, schedule_interval을 30분으로 설정
- task 구성: Extract → Load
- PythonOperator 사용
- 재시도, 실패 로그 설정

[산출물]
- 완성된 Airflow DAG 전체 코드 (단일 파일)

---

### STEP 7. 최종 검증
- 전체 파이프라인이 중복 없이 30분 주기로 동작하는지 점검
- 확장 포인트(추후 분석 컬럼 추가 가능성) 제시

[산출물]
- 파이프라인 요약
- 운영 시 주의사항

==============================
[출력 규칙]
==============================
- 각 STEP은 반드시 순서대로 출력
- 코드 블록은 실행 가능한 상태여야 함
- 불필요한 설명, 사과, 변명 금지
- “다음 단계로 넘어가겠습니다” 같은 메타 발언 금지

이제 STEP 1부터 시작하라.
